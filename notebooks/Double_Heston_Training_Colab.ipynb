{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1b05f06",
   "metadata": {},
   "source": [
    "# Double Heston + Jumps Calibration - Neural Network Training\n",
    "## Google Colab Training Notebook\n",
    "\n",
    "**Setup Instructions:**\n",
    "1. Runtime â†’ Change runtime type â†’ **T4 GPU**\n",
    "2. Run Cell 1 to upload `synthetic_10k.pkl`\n",
    "3. Run all cells sequentially\n",
    "4. Download trained model files at the end\n",
    "\n",
    "**Expected Training Time:** ~30 minutes on T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3597383",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Upload Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc253a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"ðŸ“¤ Please upload synthetic_10k.pkl (2.1 MB)\")\n",
    "print(\"   This file contains 10,000 synthetic option price samples\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "if 'synthetic_10k.pkl' not in uploaded:\n",
    "    raise FileNotFoundError(\"âŒ synthetic_10k.pkl not found. Please upload it.\")\n",
    "\n",
    "print(\"\\nâœ“ File uploaded successfully!\")\n",
    "print(f\"  Size: {os.path.getsize('synthetic_10k.pkl') / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86bccf5",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8981c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy scipy tensorflow scikit-learn matplotlib -q\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(f\"âœ“ TensorFlow {tf.__version__}\")\n",
    "print(f\"âœ“ NumPy {np.__version__}\")\n",
    "print(f\"âœ“ GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c745d9e",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Define FFN Architecture and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c167fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class OptionDataPreprocessor:\n",
    "    \"\"\"\n",
    "    Prepare option price data for neural network training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_scaler = None\n",
    "        self.target_scaler = None\n",
    "        self.param_names = [\n",
    "            'v1_0', 'kappa1', 'theta1', 'sigma1', 'rho1',\n",
    "            'v2_0', 'kappa2', 'theta2', 'sigma2', 'rho2',\n",
    "            'lambda_j', 'mu_j', 'sigma_j'\n",
    "        ]\n",
    "    \n",
    "    def extract_features(self, option_prices, strikes, maturities, spot=100.0):\n",
    "        \"\"\"\n",
    "        Extract 10 meaningful features from 15 raw option prices:\n",
    "        - 3 features per maturity (ATM, skew, butterfly)\n",
    "        - Term structure slope\n",
    "        - Total ATM premium\n",
    "        \"\"\"\n",
    "        n_samples = option_prices.shape[0]\n",
    "        n_strikes = len(strikes)\n",
    "        n_maturities = len(maturities)\n",
    "        \n",
    "        # Reshape: (n_samples, n_strikes, n_maturities)\n",
    "        prices_3d = option_prices.reshape(n_samples, n_strikes, n_maturities)\n",
    "        \n",
    "        features_list = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            sample_features = []\n",
    "            \n",
    "            for mat_idx, T in enumerate(maturities):\n",
    "                prices_at_maturity = prices_3d[i, :, mat_idx]\n",
    "                \n",
    "                # Feature 1: ATM price (normalized by spot)\n",
    "                atm_idx = np.argmin(np.abs(strikes - spot))\n",
    "                atm_price = prices_at_maturity[atm_idx]\n",
    "                sample_features.append(atm_price / spot)\n",
    "                \n",
    "                # Feature 2: Skew (25-delta risk reversal approximation)\n",
    "                otm_call_idx = np.argmin(np.abs(strikes - spot*1.05))\n",
    "                otm_put_idx = np.argmin(np.abs(strikes - spot*0.95))\n",
    "                skew = (prices_at_maturity[otm_call_idx] - \n",
    "                       prices_at_maturity[otm_put_idx]) / spot\n",
    "                sample_features.append(skew)\n",
    "                \n",
    "                # Feature 3: Curvature (butterfly)\n",
    "                itm_idx = np.argmin(np.abs(strikes - spot*0.95))\n",
    "                otm_idx = np.argmin(np.abs(strikes - spot*1.05))\n",
    "                butterfly = (prices_at_maturity[itm_idx] + \n",
    "                           prices_at_maturity[otm_idx] - \n",
    "                           2 * atm_price) / spot\n",
    "                sample_features.append(butterfly)\n",
    "            \n",
    "            # Feature 4: Term structure slope\n",
    "            if n_maturities > 1:\n",
    "                atm_short = prices_3d[i, atm_idx, 0]\n",
    "                atm_long = prices_3d[i, atm_idx, -1]\n",
    "                term_slope = (atm_long - atm_short) / spot\n",
    "                sample_features.append(term_slope)\n",
    "            \n",
    "            # Feature 5: Total ATM premium across maturities\n",
    "            total_atm = np.sum(prices_3d[i, atm_idx, :]) / spot\n",
    "            sample_features.append(total_atm)\n",
    "            \n",
    "            features_list.append(sample_features)\n",
    "        \n",
    "        return np.array(features_list)\n",
    "    \n",
    "    def prepare_training_data(self, data_path, test_size=0.15, val_size=0.15):\n",
    "        \"\"\"\n",
    "        Load synthetic data and prepare train/val/test splits\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        print(\"Loading synthetic data...\")\n",
    "        with open(data_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Extract features\n",
    "        print(\"Extracting features from option prices...\")\n",
    "        features = self.extract_features(\n",
    "            data['option_prices'],\n",
    "            data['strikes'],\n",
    "            data['maturities']\n",
    "        )\n",
    "        \n",
    "        targets = data['parameters']\n",
    "        \n",
    "        print(f\"Feature shape: {features.shape}\")\n",
    "        print(f\"Target shape: {targets.shape}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "            features, targets, test_size=(test_size + val_size), random_state=42\n",
    "        )\n",
    "        \n",
    "        val_ratio = val_size / (test_size + val_size)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(\n",
    "            X_temp, y_temp, test_size=(1 - val_ratio), random_state=42\n",
    "        )\n",
    "        \n",
    "        # Normalize features\n",
    "        print(\"Normalizing features...\")\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        X_train_scaled = self.feature_scaler.fit_transform(X_train)\n",
    "        X_val_scaled = self.feature_scaler.transform(X_val)\n",
    "        X_test_scaled = self.feature_scaler.transform(X_test)\n",
    "        \n",
    "        # Transform targets (log for positive parameters)\n",
    "        print(\"Normalizing targets...\")\n",
    "        y_train_transformed = self._transform_targets(y_train)\n",
    "        y_val_transformed = self._transform_targets(y_val)\n",
    "        y_test_transformed = self._transform_targets(y_test)\n",
    "        \n",
    "        self.target_scaler = StandardScaler()\n",
    "        y_train_scaled = self.target_scaler.fit_transform(y_train_transformed)\n",
    "        y_val_scaled = self.target_scaler.transform(y_val_transformed)\n",
    "        y_test_scaled = self.target_scaler.transform(y_test_transformed)\n",
    "        \n",
    "        print(f\"\\nData splits:\")\n",
    "        print(f\"  Train: {X_train_scaled.shape[0]} samples\")\n",
    "        print(f\"  Val:   {X_val_scaled.shape[0]} samples\")\n",
    "        print(f\"  Test:  {X_test_scaled.shape[0]} samples\")\n",
    "        \n",
    "        return {\n",
    "            'train': (X_train_scaled, y_train_scaled),\n",
    "            'val': (X_val_scaled, y_val_scaled),\n",
    "            'test': (X_test_scaled, y_test_scaled),\n",
    "            'y_train_orig': y_train,\n",
    "            'y_val_orig': y_val,\n",
    "            'y_test_orig': y_test\n",
    "        }\n",
    "    \n",
    "    def _transform_targets(self, targets):\n",
    "        \"\"\"Apply log transform to positive parameters\"\"\"\n",
    "        transformed = targets.copy()\n",
    "        log_indices = [0, 1, 2, 3, 5, 6, 7, 8, 10, 12]\n",
    "        transformed[:, log_indices] = np.log(transformed[:, log_indices] + 1e-10)\n",
    "        return transformed\n",
    "    \n",
    "    def inverse_transform_predictions(self, y_scaled):\n",
    "        \"\"\"Convert scaled predictions back to original parameter space\"\"\"\n",
    "        y_transformed = self.target_scaler.inverse_transform(y_scaled)\n",
    "        y_original = y_transformed.copy()\n",
    "        log_indices = [0, 1, 2, 3, 5, 6, 7, 8, 10, 12]\n",
    "        y_original[:, log_indices] = np.exp(y_transformed[:, log_indices])\n",
    "        return y_original\n",
    "\n",
    "\n",
    "def ffn_model(input_dim, output_dim):\n",
    "    \"\"\"Build FFN architecture: 512â†’256â†’128â†’64 with BatchNorm and Dropout\"\"\"\n",
    "    from tensorflow.keras import layers\n",
    "    \n",
    "    input_layer = layers.Input(shape=(input_dim,), name='option_features')\n",
    "    \n",
    "    x = layers.Dense(512, activation='relu')(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    \n",
    "    output_layer = layers.Dense(output_dim, activation='linear', name='model_output')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "print(\"âœ“ FFN architecture and preprocessing defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6249a9e",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cbabda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PREPARING DATA FOR TRAINING\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "preprocessor = OptionDataPreprocessor()\n",
    "data_splits = preprocessor.prepare_training_data('synthetic_10k.pkl')\n",
    "\n",
    "X_train, y_train = data_splits['train']\n",
    "X_val, y_val = data_splits['val']\n",
    "X_test, y_test = data_splits['test']\n",
    "\n",
    "print(\"\\nâœ“ Data preparation complete!\")\n",
    "print(f\"  Input features:  {X_train.shape[1]} dimensions\")\n",
    "print(f\"  Output targets:  {y_train.shape[1]} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3280e41",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: Build and Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86fbebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"BUILDING NEURAL NETWORK MODEL\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "\n",
    "model = ffn_model(input_dim=input_dim, output_dim=output_dim)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae', 'mape']\n",
    ")\n",
    "\n",
    "print(f\"Architecture: {input_dim} â†’ [512, 256, 128, 64] â†’ {output_dim}\")\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "model.summary()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b50a4",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: Train Model (~ 30 minutes on T4 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f5555",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Callbacks\n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=7,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=256,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ“ TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4148968",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c2ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_scaled = model.predict(X_test, verbose=0)\n",
    "y_pred = preprocessor.inverse_transform_predictions(y_pred_scaled)\n",
    "y_true = data_splits['y_test_orig']\n",
    "\n",
    "# Compute per-parameter errors\n",
    "param_names = preprocessor.param_names\n",
    "\n",
    "print(f\"{'Parameter':<12} {'MAE':<12} {'RMSE':<12} {'MAPE %':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, name in enumerate(param_names):\n",
    "    mae = np.mean(np.abs(y_true[:, i] - y_pred[:, i]))\n",
    "    rmse = np.sqrt(np.mean((y_true[:, i] - y_pred[:, i])**2))\n",
    "    mape = np.mean(np.abs((y_true[:, i] - y_pred[:, i]) / (y_true[:, i] + 1e-10))) * 100\n",
    "    \n",
    "    print(f\"{name:<12} {mae:<12.6f} {rmse:<12.6f} {mape:<10.2f}\")\n",
    "\n",
    "# Overall metrics\n",
    "mae_overall = np.mean(np.abs(y_true - y_pred))\n",
    "rmse_overall = np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "mape_overall = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'OVERALL':<12} {mae_overall:<12.6f} {rmse_overall:<12.6f} {mape_overall:<10.2f}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e346b",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8: Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss (MSE)', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE curve\n",
    "axes[1].plot(history.history['mae'], label='Train MAE', linewidth=2)\n",
    "axes[1].plot(history.history['val_mae'], label='Val MAE', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('MAE', fontsize=12)\n",
    "axes[1].set_title('Training and Validation MAE', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Training history plot saved to 'training_history.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4885bf",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9: Save Scalers and History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543998a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scalers\n",
    "with open('scalers.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'feature_scaler': preprocessor.feature_scaler,\n",
    "        'target_scaler': preprocessor.target_scaler,\n",
    "        'log_indices': [0, 1, 2, 3, 5, 6, 7, 8, 10, 12]\n",
    "    }, f)\n",
    "\n",
    "print(\"âœ“ Scalers saved to 'scalers.pkl'\")\n",
    "\n",
    "# Save training history\n",
    "with open('training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "print(\"âœ“ Training history saved to 'training_history.pkl'\")\n",
    "\n",
    "# Check files\n",
    "import os\n",
    "print(\"\\nFiles ready for download:\")\n",
    "for fname in ['best_model.keras', 'scalers.pkl', 'training_history.pkl', 'training_history.png']:\n",
    "    if os.path.exists(fname):\n",
    "        size_kb = os.path.getsize(fname) / 1024\n",
    "        print(f\"  âœ“ {fname:<30} ({size_kb:.1f} KB)\")\n",
    "    else:\n",
    "        print(f\"  âœ— {fname:<30} (NOT FOUND)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb423652",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10: Download All Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b24c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"ðŸ“¥ DOWNLOADING FILES ONE BY ONE\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Download files individually with delays\n",
    "files_to_download = ['best_model.keras', 'scalers.pkl', 'training_history.pkl', 'training_history.png']\n",
    "\n",
    "for i, fname in enumerate(files_to_download, 1):\n",
    "    if os.path.exists(fname):\n",
    "        size_kb = os.path.getsize(fname) / 1024\n",
    "        print(f\"[{i}/4] Downloading {fname} ({size_kb:.1f} KB)...\")\n",
    "        files.download(fname)\n",
    "        time.sleep(2)  # Wait between downloads\n",
    "        print(f\"     âœ“ {fname} downloaded!\\n\")\n",
    "    else:\n",
    "        print(f\"[{i}/4] âœ— {fname} NOT FOUND - skipping\\n\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"âœ“ DOWNLOAD COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nðŸ“‹ Next steps:\")\n",
    "print(\"  1. Check your Downloads folder for these files:\")\n",
    "print(\"     - best_model.keras\")\n",
    "print(\"     - scalers.pkl\")\n",
    "print(\"     - training_history.pkl\")\n",
    "print(\"     - training_history.png\")\n",
    "print(\"\\n  2. Rename: best_model.keras â†’ best_ffn_model.keras\")\n",
    "print(\"\\n  3. Move all files to: /Users/zen/double-heston-calibrator/\")\n",
    "print(\"\\n  4. Run fine-tuning:\")\n",
    "print(\"     python3 finetune_ffn_on_lbfgs.py\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e0edf3",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 11: Alternative Download (ZIP file) - Use if Cell 10 fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1c9e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "print(\"ðŸ“¦ Creating ZIP archive with all files...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Create zip file\n",
    "zip_filename = 'trained_model_files.zip'\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    files_to_zip = ['best_model.keras', 'scalers.pkl', 'training_history.pkl', 'training_history.png']\n",
    "    \n",
    "    for fname in files_to_zip:\n",
    "        if os.path.exists(fname):\n",
    "            size_kb = os.path.getsize(fname) / 1024\n",
    "            zipf.write(fname)\n",
    "            print(f\"  âœ“ Added {fname} ({size_kb:.1f} KB)\")\n",
    "        else:\n",
    "            print(f\"  âœ— {fname} NOT FOUND - skipping\")\n",
    "\n",
    "zip_size_mb = os.path.getsize(zip_filename) / 1024 / 1024\n",
    "print(f\"\\nâœ“ ZIP created: {zip_filename} ({zip_size_mb:.2f} MB)\")\n",
    "\n",
    "print(\"\\nðŸ“¥ Downloading ZIP file...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "files.download(zip_filename)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ“ DOWNLOAD COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nðŸ“‹ Next steps:\")\n",
    "print(\"  1. Extract trained_model_files.zip\")\n",
    "print(\"  2. You'll get these files:\")\n",
    "print(\"     - best_model.keras\")\n",
    "print(\"     - scalers.pkl\")\n",
    "print(\"     - training_history.pkl\")\n",
    "print(\"     - training_history.png\")\n",
    "print(\"\\n  3. Rename: best_model.keras â†’ best_ffn_model.keras\")\n",
    "print(\"\\n  4. Move all files to: /Users/zen/double-heston-calibrator/\")\n",
    "print(\"\\n  5. Run fine-tuning:\")\n",
    "print(\"     python3 finetune_ffn_on_lbfgs.py\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
